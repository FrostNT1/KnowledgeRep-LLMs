{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from dateutil import parser\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from urllib.parse import quote\n",
    "import xmltodict\n",
    "import sodapy\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load API key from .env file\n",
    "load_dotenv()\n",
    "NEWS_API_KEY = os.getenv('NEWS_API')\n",
    "if not NEWS_API_KEY:\n",
    "    raise ValueError(\"NEWS_API key not found in .env file\")\n",
    "DATAGOV_API = os.getenv('DATAGOV_API') \n",
    "if not DATAGOV_API:\n",
    "    raise ValueError(\"DATAGOV_API key not found in .env file\")\n",
    "\n",
    "# Thresholds for subjectivity and sentiment\n",
    "SUBJECTIVITY_THRESHOLD = 0.5\n",
    "SENTIMENT_THRESHOLD = 0.3\n",
    "START_DATE = (datetime.now() - pd.DateOffset(months=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "# Add these new constants\n",
    "GDELT_BASE_URL = \"https://api.gdeltproject.org/api/v2/doc/doc\"\n",
    "EU_DATA_BASE_URL = \"https://data.europa.eu/api/hub/search/datasets\"\n",
    "\n",
    "def parse_date(date_string: str) -> str:\n",
    "    \"\"\"Parse and format date strings.\"\"\"\n",
    "    return parser.parse(date_string).strftime('%Y-%m-%d')\n",
    "\n",
    "def analyze_text(text: str) -> tuple[float, float]:\n",
    "    \"\"\"Analyze subjectivity and sentiment of the text.\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.subjectivity, blob.sentiment.polarity\n",
    "\n",
    "def fetch_newsapi_articles() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch articles from NewsAPI.\"\"\"\n",
    "    url = f'https://newsapi.org/v2/everything'\n",
    "    params = {\n",
    "        'q': 'latest',\n",
    "        'language': 'en',\n",
    "        'from': START_DATE,\n",
    "        'sortBy': 'publishedAt',\n",
    "        'apiKey': NEWS_API_KEY\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    logger.debug(f\"NewsAPI Response: {response.json()}\")\n",
    "    articles = response.json().get('articles', [])\n",
    "    logger.info(f\"NewsAPI returned {len(articles)} articles\")\n",
    "    \n",
    "    data = []\n",
    "    threshold_filtered = 0\n",
    "    \n",
    "    for article in articles:\n",
    "        date = article.get('publishedAt', '')\n",
    "        source = article['source']['name']\n",
    "        text = article.get('content', '')\n",
    "        url = article.get('url', '')\n",
    "        subjectivity_score, sentiment_score = analyze_text(text)\n",
    "        if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "            threshold_filtered += 1\n",
    "            data.append({\n",
    "                'source': source,\n",
    "                'date': parse_date(date),\n",
    "                'text': text,\n",
    "                'url': url,\n",
    "                'subjectivity_score': subjectivity_score,\n",
    "                'sentiment_score': sentiment_score\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"NewsAPI: {len(articles)} total, {threshold_filtered} passed threshold filter\")\n",
    "    return data\n",
    "\n",
    "def fetch_plos_articles() -> List[Dict[str, Any]]:\n",
    "    base_url = 'https://journals.plos.org/plosone/'\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all articles using the main content area\n",
    "    articles = soup.select('main article')\n",
    "    logger.info(f\"PLOS returned {len(articles)} articles\")\n",
    "    \n",
    "    data = []\n",
    "    threshold_filtered = 0\n",
    "    \n",
    "    for article in articles:\n",
    "        # Get article title and link\n",
    "        title_element = article.select_one('h2 a')\n",
    "        if not title_element:\n",
    "            logger.debug(\"Article skipped due to missing title element.\")\n",
    "            continue\n",
    "            \n",
    "        title = title_element.get_text(strip=True)\n",
    "        # Properly format the URL by combining base_url with the relative path\n",
    "        link = f\"{base_url.rstrip('/')}{title_element['href']}\"\n",
    "        \n",
    "        # Get date from the article element\n",
    "        date_element = article.select_one('p.date')\n",
    "        if not date_element:\n",
    "            logger.debug(f\"Article '{title}' skipped due to missing date element.\")\n",
    "            continue\n",
    "        \n",
    "        date_text = date_element.get_text(strip=True)\n",
    "        date = parse_date(date_text)\n",
    "        \n",
    "        # Check if the article is from the last 3 months\n",
    "        if (datetime.now() - datetime.strptime(date, '%Y-%m-%d')).days > 90:\n",
    "            logger.debug(f\"Article '{title}' skipped because it is older than 3 months.\")\n",
    "            continue\n",
    "        \n",
    "        # Fetch full article content\n",
    "        try:\n",
    "            article_response = requests.get(link)\n",
    "            article_soup = BeautifulSoup(article_response.content, 'html.parser')\n",
    "            \n",
    "            # Get article abstract or content\n",
    "            content_element = article_soup.select_one('div.abstract')\n",
    "            text = f\"{title} - {content_element.get_text(strip=True)}\" if content_element else title\n",
    "            \n",
    "            subjectivity_score, sentiment_score = analyze_text(text)\n",
    "            if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "                threshold_filtered += 1\n",
    "                data.append({\n",
    "                    'source': 'PLOS',\n",
    "                    'date': date,\n",
    "                    'text': text,\n",
    "                    'url': link,\n",
    "                    'subjectivity_score': subjectivity_score,\n",
    "                    'sentiment_score': sentiment_score\n",
    "                })\n",
    "            else:\n",
    "                logger.debug(f\"Article '{title}' skipped due to subjectivity or sentiment score thresholds.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error fetching article content for '{title}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"PLOS: {len(articles)} total, {threshold_filtered} passed threshold filter\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_who_articles() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Scrape recent news releases from WHO.\"\"\"\n",
    "    url = 'https://www.who.int/news-room/releases'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    articles = soup.find_all('div', class_='list-view--item vertical-list-item')\n",
    "    logger.info(f\"WHO returned {len(articles)} articles\")\n",
    "    \n",
    "    data = []\n",
    "    threshold_filtered = 0\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.find('a').get_text(strip=True)\n",
    "        date_text = article.find('span', class_='timestamp').get_text(strip=True)\n",
    "        date = parse_date(date_text)\n",
    "        link = \"https://www.who.int\" + article.find('a')['href']\n",
    "        subjectivity_score, sentiment_score = analyze_text(title)\n",
    "        if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "            threshold_filtered += 1\n",
    "            data.append({\n",
    "                'source': 'WHO',\n",
    "                'date': date,\n",
    "                'text': title,\n",
    "                'url': link,\n",
    "                'subjectivity_score': subjectivity_score,\n",
    "                'sentiment_score': sentiment_score\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"WHO: {len(articles)} total, {threshold_filtered} passed threshold filter\")\n",
    "    return data\n",
    "\n",
    "def fetch_un_articles() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Scrape recent news from the United Nations.\"\"\"\n",
    "    url = 'https://www.un.org/press/en'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    articles = soup.find_all('div', class_='views-row')\n",
    "    logger.info(f\"UN returned {len(articles)} articles\")\n",
    "    \n",
    "    data = []\n",
    "    threshold_filtered = 0\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.find('a').get_text(strip=True)\n",
    "        date_text = article.find('span', class_='date-display-single').get_text(strip=True)\n",
    "        date = parse_date(date_text)\n",
    "        link = \"https://www.un.org\" + article.find('a')['href']\n",
    "        subjectivity_score, sentiment_score = analyze_text(title)\n",
    "        if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "            threshold_filtered += 1\n",
    "            data.append({\n",
    "                'source': 'UN',\n",
    "                'date': date,\n",
    "                'text': title,\n",
    "                'url': link,\n",
    "                'subjectivity_score': subjectivity_score,\n",
    "                'sentiment_score': sentiment_score\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"UN: {len(articles)} total, {threshold_filtered} passed threshold filter\")\n",
    "    return data\n",
    "\n",
    "def fetch_gdelt_articles() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch recent articles from GDELT Project.\"\"\"\n",
    "    params = {\n",
    "        'format': 'json',\n",
    "        'maxrecords': 250,\n",
    "        'timespan': '1440',  # Last 24 hours\n",
    "        'format': 'json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(GDELT_BASE_URL, params=params)\n",
    "    articles = response.json().get('articles', [])\n",
    "    \n",
    "    data = []\n",
    "    for article in articles:\n",
    "        text = f\"{article.get('title', '')} {article.get('seentext', '')}\"\n",
    "        date = parse_date(article.get('seendate', ''))\n",
    "        subjectivity_score, sentiment_score = analyze_text(text)\n",
    "        \n",
    "        if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "            data.append({\n",
    "                'source': 'GDELT',\n",
    "                'date': date,\n",
    "                'text': text,\n",
    "                'url': article.get('url', ''),\n",
    "                'subjectivity_score': subjectivity_score,\n",
    "                'sentiment_score': sentiment_score\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"GDELT: {len(articles)} total, {len(data)} passed threshold filter\")\n",
    "    return data\n",
    "\n",
    "def fetch_datagov_articles() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch recent datasets from Data.gov.\"\"\"\n",
    "    client = sodapy.Socrata(\"data.gov\", DATAGOV_API)\n",
    "    \n",
    "    # Fetch recent datasets\n",
    "    results = client.get(\"7g6j-rrh5\", limit=100, order=\"modification_date DESC\")\n",
    "    \n",
    "    data = []\n",
    "    for result in results:\n",
    "        text = f\"{result.get('title', '')} {result.get('description', '')}\"\n",
    "        date = parse_date(result.get('modification_date', ''))\n",
    "        subjectivity_score, sentiment_score = analyze_text(text)\n",
    "        \n",
    "        if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "            data.append({\n",
    "                'source': 'Data.gov',\n",
    "                'date': date,\n",
    "                'text': text,\n",
    "                'url': result.get('landingPage', ''),\n",
    "                'subjectivity_score': subjectivity_score,\n",
    "                'sentiment_score': sentiment_score\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"Data.gov: {len(results)} total, {len(data)} passed threshold filter\")\n",
    "    return data\n",
    "\n",
    "def fetch_eu_data_articles() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Fetch recent datasets from EU Open Data Portal.\"\"\"\n",
    "    params = {\n",
    "        'limit': 100,\n",
    "        'sort': 'modified',\n",
    "        'order': 'desc',\n",
    "        'format': 'json'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(EU_DATA_BASE_URL, params=params)\n",
    "    results = response.json().get('result', {}).get('results', [])\n",
    "    \n",
    "    data = []\n",
    "    for result in results:\n",
    "        text = f\"{result.get('title', '')} {result.get('description', '')}\"\n",
    "        date = parse_date(result.get('modified', ''))\n",
    "        subjectivity_score, sentiment_score = analyze_text(text)\n",
    "        \n",
    "        if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "            data.append({\n",
    "                'source': 'EU Open Data',\n",
    "                'date': date,\n",
    "                'text': text,\n",
    "                'url': result.get('landingPage', ''),\n",
    "                'subjectivity_score': subjectivity_score,\n",
    "                'sentiment_score': sentiment_score\n",
    "            })\n",
    "    \n",
    "    logger.info(f\"EU Data Portal: {len(results)} total, {len(data)} passed threshold filter\")\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_who_articles() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Scrape recent news releases from WHO with pagination.\"\"\"\n",
    "    base_url = 'https://www.who.int/news-room/releases'\n",
    "    data = []\n",
    "    threshold_filtered = 0\n",
    "    page = 1\n",
    "    max_articles = 100\n",
    "\n",
    "    while len(data) < max_articles:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            logger.info(f\"Failed to retrieve page {page}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the main container\n",
    "        main_container = soup.select_one('div.sf-list-vertical')\n",
    "        if not main_container:\n",
    "            logger.info(\"No main container found\")\n",
    "            break\n",
    "            \n",
    "        # Find all article rows\n",
    "        article_rows = main_container.find_all('div', class_='list-view--item', recursive=False)\n",
    "        if not article_rows:\n",
    "            logger.info(f\"No articles found on page {page}\")\n",
    "            break\n",
    "            \n",
    "        logger.info(f\"WHO page {page} returned {len(article_rows)} articles\")\n",
    "        \n",
    "        for row in article_rows:\n",
    "            # Extract link\n",
    "            link_elem = row.find('a')\n",
    "            if not link_elem or 'href' not in link_elem.attrs:\n",
    "                continue\n",
    "            link = \"https://www.who.int\" + link_elem['href']\n",
    "            \n",
    "            # Extract date\n",
    "            date_elem = row.select_one('div.timestamp span')\n",
    "            if not date_elem:\n",
    "                continue\n",
    "            date_text = date_elem.get_text(strip=True)\n",
    "            date = parse_date(date_text)  # This will handle \"7 November 2024\" format\n",
    "            \n",
    "            # Extract title\n",
    "            title = link_elem.get_text(strip=True).replace(date_text, '').strip()\n",
    "            \n",
    "            # Check if article is within 3 months\n",
    "            if (datetime.now() - datetime.strptime(date, '%Y-%m-%d')).days > 90:\n",
    "                logger.info(\"Reached articles older than 3 months\")\n",
    "                return data\n",
    "            \n",
    "            # Analyze text sentiment and subjectivity\n",
    "            subjectivity_score, sentiment_score = analyze_text(title)\n",
    "            \n",
    "            # Filter based on thresholds\n",
    "            if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "                threshold_filtered += 1\n",
    "                data.append({\n",
    "                    'source': 'WHO',\n",
    "                    'date': date,\n",
    "                    'text': title,\n",
    "                    'url': link,\n",
    "                    'subjectivity_score': subjectivity_score,\n",
    "                    'sentiment_score': sentiment_score\n",
    "                })\n",
    "                \n",
    "                if len(data) >= max_articles:\n",
    "                    logger.info(\"Reached maximum number of articles\")\n",
    "                    return data\n",
    "        \n",
    "        page += 1\n",
    "    \n",
    "    logger.info(f\"WHO: Total articles processed across {page-1} pages, {threshold_filtered} passed threshold filter\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_who_articles() -> List[Dict[str, Any]]:\n",
    "    \"\"\"Scrape recent news releases from WHO with pagination.\"\"\"\n",
    "    base_url = 'https://www.who.int/news-room/releases'\n",
    "    data = []\n",
    "    threshold_filtered = 0\n",
    "    page = 1\n",
    "    max_articles = 100\n",
    "\n",
    "    while len(data) < max_articles:\n",
    "        url = f\"{base_url}?page={page}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            logger.info(f\"Failed to retrieve page {page}\")\n",
    "            break\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Find the main container\n",
    "        main_container = soup.select_one('div.sf-list-vertical')\n",
    "        if not main_container:\n",
    "            logger.info(\"No main container found\")\n",
    "            break\n",
    "            \n",
    "        # Find all article rows\n",
    "        article_rows = main_container.find_all('div', class_='list-view--item', recursive=False)\n",
    "        if not article_rows:\n",
    "            logger.info(f\"No articles found on page {page}\")\n",
    "            break\n",
    "            \n",
    "        logger.info(f\"WHO page {page} returned {len(article_rows)} articles\")\n",
    "        \n",
    "        for row in article_rows:\n",
    "            # Extract link\n",
    "            link_elem = row.find('a')\n",
    "            if not link_elem or 'href' not in link_elem.attrs:\n",
    "                continue\n",
    "            link = \"https://www.who.int\" + link_elem['href']\n",
    "            \n",
    "            # Extract date\n",
    "            date_elem = row.select_one('div.timestamp span')\n",
    "            if not date_elem:\n",
    "                continue\n",
    "            date_text = date_elem.get_text(strip=True)\n",
    "            date = parse_date(date_text)  # This will handle \"7 November 2024\" format\n",
    "            \n",
    "            # Extract title\n",
    "            title = link_elem.get_text(strip=True).replace(date_text, '').strip()\n",
    "            \n",
    "            # Check if article is within 3 months\n",
    "            if (datetime.now() - datetime.strptime(date, '%Y-%m-%d')).days > 90:\n",
    "                logger.info(\"Reached articles older than 3 months\")\n",
    "                return data\n",
    "            \n",
    "            # Analyze text sentiment and subjectivity\n",
    "            subjectivity_score, sentiment_score = analyze_text(title)\n",
    "            \n",
    "            # Filter based on thresholds\n",
    "            if subjectivity_score <= SUBJECTIVITY_THRESHOLD and abs(sentiment_score) <= SENTIMENT_THRESHOLD:\n",
    "                threshold_filtered += 1\n",
    "                data.append({\n",
    "                    'source': 'WHO',\n",
    "                    'date': date,\n",
    "                    'text': title,\n",
    "                    'url': link,\n",
    "                    'subjectivity_score': subjectivity_score,\n",
    "                    'sentiment_score': sentiment_score\n",
    "                })\n",
    "                \n",
    "                if len(data) >= max_articles:\n",
    "                    logger.info(\"Reached maximum number of articles\")\n",
    "                    return data\n",
    "        \n",
    "        page += 1\n",
    "    \n",
    "    logger.info(f\"WHO: Total articles processed across {page-1} pages, {threshold_filtered} passed threshold filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div', class_='k-listview-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected_data_20241108_151603.parquet\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = os.getenv('DATA_PATH')\n",
    "!ls $DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "      <th>subjectivity_score</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Financial Post</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>Author of the article:\\r\\nArticle content\\r\\nC...</td>\n",
       "      <td>https://financialpost.com/globe-newswire/freeh...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Smithsonian.com</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>Shilo Shiv Suleman's Padma/Lotus is the first ...</td>\n",
       "      <td>https://www.smithsonianmag.com/smart-news/to-s...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bleeding Cool News</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>Posted in: Activision, Call of Duty, Call Of D...</td>\n",
       "      <td>https://bleedingcool.com/games/call-of-duty-bl...</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>-0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GamesRadar+</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>Dragon Ball: Sparking Zero has become a big an...</td>\n",
       "      <td>https://www.gamesradar.com/games/fighting/drag...</td>\n",
       "      <td>0.375758</td>\n",
       "      <td>0.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Financial Post</td>\n",
       "      <td>2024-11-07</td>\n",
       "      <td>Lightspeed Commerce Inc.s chief executive says...</td>\n",
       "      <td>https://financialpost.com/pmn/lightspeed-wants...</td>\n",
       "      <td>0.435859</td>\n",
       "      <td>-0.004798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               source        date  \\\n",
       "0      Financial Post  2024-11-07   \n",
       "1     Smithsonian.com  2024-11-07   \n",
       "2  Bleeding Cool News  2024-11-07   \n",
       "3         GamesRadar+  2024-11-07   \n",
       "4      Financial Post  2024-11-07   \n",
       "\n",
       "                                                text  \\\n",
       "0  Author of the article:\\r\\nArticle content\\r\\nC...   \n",
       "1  Shilo Shiv Suleman's Padma/Lotus is the first ...   \n",
       "2  Posted in: Activision, Call of Duty, Call Of D...   \n",
       "3  Dragon Ball: Sparking Zero has become a big an...   \n",
       "4  Lightspeed Commerce Inc.s chief executive says...   \n",
       "\n",
       "                                                 url  subjectivity_score  \\\n",
       "0  https://financialpost.com/globe-newswire/freeh...            0.000000   \n",
       "1  https://www.smithsonianmag.com/smart-news/to-s...            0.166667   \n",
       "2  https://bleedingcool.com/games/call-of-duty-bl...            0.433333   \n",
       "3  https://www.gamesradar.com/games/fighting/drag...            0.375758   \n",
       "4  https://financialpost.com/pmn/lightspeed-wants...            0.435859   \n",
       "\n",
       "   sentiment_score  \n",
       "0         0.000000  \n",
       "1         0.125000  \n",
       "2        -0.166667  \n",
       "3         0.272727  \n",
       "4        -0.004798  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(os.path.join(DATA_PATH, 'collected_data_20241108_151603.parquet'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/knowledge-llms/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Clean the text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = text.encode('ascii', 'ignore').decode()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text.lower()\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "df.loc[:, 'clean_text'] = df.loc[:, 'text'].apply(clean_text)\n",
    "df.head()\n",
    "\n",
    "df = df.drop_duplicates(subset=['clean_text'])\n",
    "\n",
    "df.loc[:, 'is_english'] = df.loc[:, 'clean_text'].apply(is_english)\n",
    "df = df[df['is_english']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qa_pair(text):\n",
    "    # Simplified placeholder function\n",
    "    # Needs to be adapted based on actual text patterns\n",
    "    parts = text.split(' ')\n",
    "    if len(parts) > 4:\n",
    "        question = ' '.join(parts[:4]) + '?'\n",
    "        answer = ' '.join(parts[4:])\n",
    "        return {'question': question, 'answer': answer}\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df['qa_pair'] = df['clean_text'].apply(create_qa_pair)\n",
    "df = df.dropna(subset=['qa_pair'])\n",
    "\n",
    "df['question'] = df['qa_pair'].apply(lambda x: x['question'])\n",
    "df['answer'] = df['qa_pair'].apply(lambda x: x['answer'])\n",
    "\n",
    "qa_df = df[['question', 'answer']]\n",
    "\n",
    "# Split the QA data\n",
    "qa_train, qa_temp = train_test_split(qa_df, test_size=0.2, random_state=42)\n",
    "qa_val, qa_test = train_test_split(qa_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# # Save QA datasets\n",
    "# qa_train.to_json('qa_train.jsonl', orient='records', lines=True)\n",
    "# qa_val.to_json('qa_validation.jsonl', orient='records', lines=True)\n",
    "# qa_test.to_json('qa_test.jsonl', orient='records', lines=True)\n",
    "\n",
    "# # Create Hugging Face datasets for QA\n",
    "# train_dataset = Dataset.from_pandas(qa_train)\n",
    "# val_dataset = Dataset.from_pandas(qa_val)\n",
    "# test_dataset = Dataset.from_pandas(qa_test)\n",
    "\n",
    "# qa_dataset = DatasetDict({\n",
    "#     'train': train_dataset,\n",
    "#     'validation': val_dataset,\n",
    "#     'test': test_dataset\n",
    "# })\n",
    "\n",
    "# # Save QA datasets\n",
    "# qa_dataset.save_to_disk('qa_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
